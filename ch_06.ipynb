{"cells":[{"cell_type":"markdown","source":["# 6. MDP를 모를 때 최고의 정책 찾기\n","바닥부터 배우는 강화 학습 6장에 있는 코드를 참고 했습니다."],"metadata":{"id":"DHOtaI_Jpity"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1pwHz0daovSu"},"outputs":[],"source":["import random\n","import numpy as np\n","\n","from tqdm import trange"]},{"cell_type":"markdown","metadata":{"id":"wO86V8lOovSx"},"source":["# 6.1 몬테카를로 컨트롤"]},{"cell_type":"markdown","metadata":{"id":"CULCyJ7wovSy"},"source":["- GridWorld 클래스"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f8eAOAhYovSy"},"outputs":[],"source":["class GridWorld():\n","    def __init__(self):\n","        self.x=0\n","        self.y=0\n","    \n","    def step(self, a):\n","        # 0번 액션: 왼쪽, 1번 액션: 위, 2번 액션: 오른쪽, 3번 액션: 아래쪽\n","        if a==0:\n","            self.move_left()\n","        elif a==1:\n","            self.move_up()\n","        elif a==2:\n","            self.move_right()\n","        elif a==3:\n","            self.move_down()\n","\n","        reward = -1  # 보상은 항상 -1로 고정\n","        done = self.is_done()\n","        return (self.x, self.y), reward, done\n","\n","    def move_left(self):\n","        if self.y==0:\n","            pass\n","        elif self.y==3 and self.x in [0,1,2]:\n","            pass\n","        elif self.y==5 and self.x in [2,3,4]:\n","            pass\n","        else:\n","            self.y -= 1\n","\n","    def move_right(self):\n","        if self.y==1 and self.x in [0,1,2]:\n","            pass\n","        elif self.y==3 and self.x in [2,3,4]:\n","            pass\n","        elif self.y==6:\n","            pass\n","        else:\n","            self.y += 1\n","      \n","    def move_up(self):\n","        if self.x==0:\n","            pass\n","        elif self.x==3 and self.y==2:\n","            pass\n","        else:\n","            self.x -= 1\n","\n","    def move_down(self):\n","        if self.x==4:\n","            pass\n","        elif self.x==1 and self.y==4:\n","            pass\n","        else:\n","            self.x+=1\n","\n","    def is_done(self):\n","        if self.x==4 and self.y==6: # 목표 지점인 (4,6)에 도달하면 끝난다\n","            return True\n","        else:\n","            return False\n","      \n","    def reset(self):\n","        self.x = 0\n","        self.y = 0\n","        return (self.x, self.y)"]},{"cell_type":"markdown","metadata":{"id":"2JNvzTnuovSz"},"source":["- Agent 클래스"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8P76rn1VovS0"},"outputs":[],"source":["class QAgent():\n","    def __init__(self):\n","        self.q_table = np.zeros((5, 7, 4)) # q벨류를 저장하는 변수. 모두 0으로 초기화. \n","        self.eps = 0.9 \n","        self.alpha = 0.01\n","        \n","    def select_action(self, s):\n","        # eps-greedy로 액션을 선택\n","        x, y = s\n","        coin = random.random()\n","        if coin < self.eps:\n","            action = random.randint(0, 3)\n","        else:\n","            action_val = self.q_table[x,y,:]\n","            action = np.argmax(action_val)\n","        return action\n","\n","    def update_table(self, history):\n","        # 한 에피소드에 해당하는 history를 입력으로 받아 q 테이블의 값을 업데이트 한다\n","        cum_reward = 0\n","        for transition in history[::-1]:\n","            s, a, r, s_prime = transition\n","            x,y = s\n","            # 몬테 카를로 방식을 이용하여 업데이트.\n","            self.q_table[x,y,a] = self.q_table[x,y,a] + self.alpha * (cum_reward - self.q_table[x,y,a])\n","            cum_reward = cum_reward + r \n","\n","    def anneal_eps(self):\n","        self.eps -= 0.03\n","        self.eps = max(self.eps, 0.1)\n","\n","    def show_table(self):\n","        # 학습이 각 위치에서 어느 액션의 q 값이 가장 높았는지 보여주는 함수\n","        q_lst = self.q_table.tolist()\n","        data = np.zeros((5,7))\n","        for row_idx in range(len(q_lst)):\n","            row = q_lst[row_idx]\n","            for col_idx in range(len(row)):\n","                col = row[col_idx]\n","                action = np.argmax(col)\n","                data[row_idx, col_idx] = action\n","        print(data)"]},{"cell_type":"markdown","metadata":{"id":"fu5p9uEwovS1"},"source":["- 초기 환경"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rs_HwZGfovS1"},"outputs":[],"source":["env = GridWorld()\n","agent = QAgent()"]},{"cell_type":"markdown","metadata":{"id":"P_iJ_TuqovS2"},"source":["- 1천번 에피소드 진행"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HgLYaiUNovS2","executionInfo":{"status":"ok","timestamp":1672973980683,"user_tz":-540,"elapsed":390,"user":{"displayName":"현청천","userId":"02662570985009482782"}},"outputId":"8dce5380-4f74-44fc-dfbd-4d1c4236d4e1"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:00<00:00, 3115.56it/s]\n"]}],"source":["for k in trange(1000):\n","    done = False\n","    history = []\n","    # env 초기화\n","    s = env.reset()\n","    # 에피소드 1회 진행\n","    while not done:\n","        a = agent.select_action(s)\n","        s_prime, r, done = env.step(a)\n","        history.append((s, a, r, s_prime))\n","        s = s_prime\n","    # 에피소드 종료 후 테이블 업데이트\n","    agent.update_table(history)\n","    agent.anneal_eps()"]},{"cell_type":"markdown","metadata":{"id":"Bxm5kF_QovS3"},"source":["- 학습이 끝난 후 데이터 출력해보기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fVh70oM6ovS3","executionInfo":{"status":"ok","timestamp":1672973980683,"user_tz":-540,"elapsed":5,"user":{"displayName":"현청천","userId":"02662570985009482782"}},"outputId":"a41701f1-5b02-4f8d-f3dd-947ac0de46bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[2. 3. 0. 2. 2. 3. 3.]\n"," [3. 0. 0. 1. 1. 3. 3.]\n"," [3. 3. 0. 1. 0. 3. 3.]\n"," [3. 0. 2. 1. 0. 3. 3.]\n"," [2. 2. 1. 0. 0. 2. 0.]]\n"]}],"source":["agent.show_table()"]},{"cell_type":"markdown","metadata":{"id":"JyDirUvUovS6"},"source":["# 6.2 TD 컨트롤 1 - SARSA"]},{"cell_type":"markdown","metadata":{"id":"6vQVM5P-1kaD"},"source":["- GridWorld 클래스"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"34a_VinJ1kaD"},"outputs":[],"source":["class GridWorld():\n","    def __init__(self):\n","        self.x=0\n","        self.y=0\n","    \n","    def step(self, a):\n","        # 0번 액션: 왼쪽, 1번 액션: 위, 2번 액션: 오른쪽, 3번 액션: 아래쪽\n","        if a==0:\n","            self.move_left()\n","        elif a==1:\n","            self.move_up()\n","        elif a==2:\n","            self.move_right()\n","        elif a==3:\n","            self.move_down()\n","\n","        reward = -1  # 보상은 항상 -1로 고정\n","        done = self.is_done()\n","        return (self.x, self.y), reward, done\n","\n","    def move_left(self):\n","        if self.y==0:\n","            pass\n","        elif self.y==3 and self.x in [0,1,2]:\n","            pass\n","        elif self.y==5 and self.x in [2,3,4]:\n","            pass\n","        else:\n","            self.y -= 1\n","\n","    def move_right(self):\n","        if self.y==1 and self.x in [0,1,2]:\n","            pass\n","        elif self.y==3 and self.x in [2,3,4]:\n","            pass\n","        elif self.y==6:\n","            pass\n","        else:\n","            self.y += 1\n","      \n","    def move_up(self):\n","        if self.x==0:\n","            pass\n","        elif self.x==3 and self.y==2:\n","            pass\n","        else:\n","            self.x -= 1\n","\n","    def move_down(self):\n","        if self.x==4:\n","            pass\n","        elif self.x==1 and self.y==4:\n","            pass\n","        else:\n","            self.x+=1\n","\n","    def is_done(self):\n","        if self.x==4 and self.y==6: # 목표 지점인 (4,6)에 도달하면 끝난다\n","            return True\n","        else:\n","            return False\n","      \n","    def reset(self):\n","        self.x = 0\n","        self.y = 0\n","        return (self.x, self.y)"]},{"cell_type":"markdown","metadata":{"id":"vhfWxwZb1kaE"},"source":["- Agent 클래스"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pn5i-oCh1kaE"},"outputs":[],"source":["class QAgent():\n","    def __init__(self):\n","        self.q_table = np.zeros((5, 7, 4)) # 마찬가지로 Q 테이블을 0으로 초기화\n","        self.eps = 0.9\n","\n","    def select_action(self, s):\n","        # eps-greedy로 액션을 선택해준다\n","        x, y = s\n","        coin = random.random()\n","        if coin < self.eps:\n","            action = random.randint(0,3)\n","        else:\n","            action_val = self.q_table[x,y,:]\n","            action = np.argmax(action_val)\n","        return action\n","\n","    def update_table(self, transition):\n","        s, a, r, s_prime = transition\n","        x,y = s\n","        next_x, next_y = s_prime\n","        a_prime = self.select_action(s_prime) # S'에서 선택할 액션 (실제로 취한 액션이 아님)\n","        # SARSA 업데이트 식을 이용\n","        self.q_table[x,y,a] = self.q_table[x,y,a] + 0.1 * (r + self.q_table[next_x,next_y,a_prime] - self.q_table[x,y,a])\n","\n","    def anneal_eps(self):\n","        self.eps -= 0.03\n","        self.eps = max(self.eps, 0.1)\n","\n","    def show_table(self):\n","        q_lst = self.q_table.tolist()\n","        data = np.zeros((5,7))\n","        for row_idx in range(len(q_lst)):\n","            row = q_lst[row_idx]\n","            for col_idx in range(len(row)):\n","                col = row[col_idx]\n","                action = np.argmax(col)\n","                data[row_idx, col_idx] = action\n","        print(data)"]},{"cell_type":"markdown","metadata":{"id":"zHs4iJ5J1kaE"},"source":["- 초기 환경"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VS6hN6MO1kaF"},"outputs":[],"source":["env = GridWorld()\n","agent = QAgent()"]},{"cell_type":"markdown","metadata":{"id":"-F0UV0ui1kaF"},"source":["- 1천번 에피소드 진행"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672973981022,"user_tz":-540,"elapsed":19,"user":{"displayName":"현청천","userId":"02662570985009482782"}},"outputId":"6fa8c126-00d8-450a-8c69-eecb8f363160","id":"c4xv1UWZ1kaF"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:00<00:00, 4731.48it/s]\n"]}],"source":["for k in trange(1000):\n","    done = False\n","    \n","    s = env.reset()\n","    while not done:\n","        a = agent.select_action(s)\n","        s_prime, r, done = env.step(a)\n","        agent.update_table((s,a,r,s_prime))\n","        s = s_prime\n","    agent.anneal_eps()"]},{"cell_type":"markdown","metadata":{"id":"G4JVKdKp1kaF"},"source":["- 학습이 끝난 후 데이터 출력해보기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672973981022,"user_tz":-540,"elapsed":17,"user":{"displayName":"현청천","userId":"02662570985009482782"}},"outputId":"be1da067-5a7a-4724-9dac-7b6dbdf55f94","id":"br0YSHUx1kaF"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[3. 3. 0. 0. 2. 2. 3.]\n"," [3. 3. 0. 2. 2. 3. 3.]\n"," [3. 3. 0. 1. 0. 3. 3.]\n"," [2. 2. 2. 1. 0. 3. 3.]\n"," [0. 2. 2. 3. 0. 2. 0.]]\n"]}],"source":["agent.show_table()"]},{"cell_type":"markdown","metadata":{"id":"Oj5WK95hAu2n"},"source":["# 6.3 TD 컨트롤 2 - Q러닝"]},{"cell_type":"markdown","metadata":{"id":"8C829uuaAu2o"},"source":["- GridWorld 클래스"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m5G3NXzDAu2o"},"outputs":[],"source":["class GridWorld():\n","    def __init__(self):\n","        self.x=0\n","        self.y=0\n","    \n","    def step(self, a):\n","        # 0번 액션: 왼쪽, 1번 액션: 위, 2번 액션: 오른쪽, 3번 액션: 아래쪽\n","        if a==0:\n","            self.move_left()\n","        elif a==1:\n","            self.move_up()\n","        elif a==2:\n","            self.move_right()\n","        elif a==3:\n","            self.move_down()\n","\n","        reward = -1 # 보상은 항상 -1로 고정\n","        done = self.is_done()\n","        return (self.x, self.y), reward, done\n","\n","    def move_left(self):\n","        if self.y==0:\n","            pass\n","        elif self.y==3 and self.x in [0,1,2]:\n","            pass\n","        elif self.y==5 and self.x in [2,3,4]:\n","            pass\n","        else:\n","            self.y -= 1\n","\n","    def move_right(self):\n","        if self.y==1 and self.x in [0,1,2]:\n","            pass\n","        elif self.y==3 and self.x in [2,3,4]:\n","            pass\n","        elif self.y==6:\n","            pass\n","        else:\n","            self.y += 1\n","      \n","    def move_up(self):\n","        if self.x==0:\n","            pass\n","        elif self.x==3 and self.y==2:\n","            pass\n","        else:\n","            self.x -= 1\n","\n","    def move_down(self):\n","        if self.x==4:\n","            pass\n","        elif self.x==1 and self.y==4:\n","            pass\n","        else:\n","            self.x+=1\n","\n","    def is_done(self):\n","        if self.x==4 and self.y==6:\n","            return True\n","        else:\n","            return False\n","      \n","    def reset(self):\n","        self.x = 0\n","        self.y = 0\n","        return (self.x, self.y)"]},{"cell_type":"markdown","metadata":{"id":"9wDhlHwVAu2o"},"source":["- Agent 클래스"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9TU_0QxYAu2o"},"outputs":[],"source":["class QAgent():\n","    def __init__(self):\n","        self.q_table = np.zeros((5, 7, 4)) # 마찬가지로 Q 테이블을 0으로 초기화\n","        self.eps = 0.9\n","\n","    def select_action(self, s):\n","        # eps-greedy로 액션을 선택해준다\n","        x, y = s\n","        coin = random.random()\n","        if coin < self.eps:\n","            action = random.randint(0,3)\n","        else:\n","            action_val = self.q_table[x,y,:]\n","            action = np.argmax(action_val)\n","        return action\n","\n","    def update_table(self, transition):\n","        s, a, r, s_prime = transition\n","        x,y = s\n","        next_x, next_y = s_prime\n","        a_prime = self.select_action(s_prime) # S'에서 선택할 액션 (실제로 취한 액션이 아님)\n","        # Q러닝 업데이트 식을 이용 \n","        self.q_table[x,y,a] = self.q_table[x,y,a] + 0.1 * (r + np.amax(self.q_table[next_x,next_y,:]) - self.q_table[x,y,a])\n","\n","    def anneal_eps(self):\n","        self.eps -= 0.01  # Q러닝에선 epsilon 이 좀더 천천히 줄어 들도록 함.\n","        self.eps = max(self.eps, 0.2) \n","\n","    def show_table(self):\n","        q_lst = self.q_table.tolist()\n","        data = np.zeros((5,7))\n","        for row_idx in range(len(q_lst)):\n","            row = q_lst[row_idx]\n","            for col_idx in range(len(row)):\n","                col = row[col_idx]\n","                action = np.argmax(col)\n","                data[row_idx, col_idx] = action\n","        print(data)"]},{"cell_type":"markdown","metadata":{"id":"o063_OQJAu2p"},"source":["- 초기 환경"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xDp9LHyCAu2p"},"outputs":[],"source":["env = GridWorld()\n","agent = QAgent()"]},{"cell_type":"markdown","metadata":{"id":"-CSD5t3UAu2p"},"source":["- 1천번 에피소드 진행"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672973981595,"user_tz":-540,"elapsed":319,"user":{"displayName":"현청천","userId":"02662570985009482782"}},"outputId":"a77b5c09-44b9-49d5-c6d5-99ed18f4389f","id":"pibUcDnJAu2p"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:00<00:00, 2052.85it/s]\n"]}],"source":["for n_epi in trange(1000):\n","    done = False\n","\n","    s = env.reset()\n","    while not done:\n","        a = agent.select_action(s)\n","        s_prime, r, done = env.step(a)\n","        agent.update_table((s,a,r,s_prime))\n","        s = s_prime\n","    agent.anneal_eps()"]},{"cell_type":"markdown","metadata":{"id":"zTC7-VIUAu2p"},"source":["- 학습이 끝난 후 데이터 출력해보기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672973981596,"user_tz":-540,"elapsed":5,"user":{"displayName":"현청천","userId":"02662570985009482782"}},"outputId":"53b2c60b-45c9-49d2-b9b1-07a6f3cee7f2","id":"NYPq0s7nAu2p"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[2. 3. 0. 2. 2. 3. 3.]\n"," [3. 3. 0. 2. 2. 3. 3.]\n"," [3. 3. 0. 1. 0. 3. 3.]\n"," [2. 2. 2. 1. 0. 2. 3.]\n"," [2. 1. 2. 1. 0. 2. 0.]]\n"]}],"source":["agent.show_table()"]},{"cell_type":"code","source":[],"metadata":{"id":"_ZaMRh9y2EtO"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.13 ('challenge')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"b4db2a12af3ef8b0e30f6de14b6f9eeee638905350cc1d2be0bebab10d16d430"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}